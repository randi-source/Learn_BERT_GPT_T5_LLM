{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamespace Variables\n",
    "\n",
    "env = gym.make(\"Pong-v0\") # environment info\n",
    "xs, rs, ys = [], [], [] # State, Reward, Action History\n",
    "average_reward = None # Reward for measuring average performance\n",
    "G = 0\n",
    "n_episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "\n",
    "n_inputs = 80**2    # size of inputs (pixel inputs)\n",
    "n_hidden = 200      # number of hidden layer neurons\n",
    "n_action = 3        # number of available actions i.e. fire (do nothing), left & right\n",
    "learning_rate = 1e-3\n",
    "gamma = .99         # discount factor for reward\n",
    "decay1, decay2 = 0.9, 0.999 # decay rate for Adam Optimizer\n",
    "save_path = \"models/pong.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = {}\n",
    "# Initialize random weights using xavier initialization\n",
    "with tf.variable_scope('weights_one', reuse=False):\n",
    "    xavier_l1 = tf.truncated_normal_initializer(mean = 0, stddev = 1./np.sqrt(n_inputs), dtype = tf.f)\n",
    "    tf_model['W1'] = tf.get_variable(\"W1\", [n_input, n_hidden], initializer = xavier_l1)\n",
    "with tf.variable_scope('weights_one', reuse=False):\n",
    "    xavier_l2 = tf.truncated_normal_initializer(mean = 0, stddev = 1./np.sqrt(n_inputs), dtype = tf.f)\n",
    "    tf_model['W2'] = tf.get_variable(\"W2\", [n_input, n_hidden], initializer = xavier_l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "# Crop, greyscale, and reshape pixels, returned flatened\n",
    "def resize(img):\n",
    "    i = img[35:195,:] # Crop out unnecesary top and bottom sections\n",
    "    i = np.round(np.mean(i, -1)) # Get greyscale & round to nearest integer\n",
    "    i[i==78] = 0 # Get rid of background\n",
    "    i[159,:] = 0 # Get rid of bottom line\n",
    "    i = scipy.misc.imresize(i, [80,80], interp = 'nearest') #Resize\n",
    "    i[i !=0] = 1 # Set everything else to 1 i.e. paddles and ball\n",
    "    return i.astype(np.float).ravel() # Returns pixels in flattened state\n",
    "\n",
    "# Apply reward discounting to a series of rewards\n",
    "def tf_discount(tf_r): # tf_r - [game_steps, 1]\n",
    "    discount_f = lambda a, v: a*gamma + v; # Function for calculating discounted reward\n",
    "    tf_r_reverse = tf.scan(discount_f, tf.reverse(tf_r,[True, False])) # Reverse and applies\n",
    "    tf_discounted_r = tf.reverse(tf_r_reverse, [True, False]) # Reverse back in descending\n",
    "    return tf_discounted_r \n",
    "\n",
    "# Feed input and calculate action using weights\n",
    "def tf_policy_forward(x): #x = [1,D]\n",
    "    h = tf.nn.relu(tf.matmul(x, tf_model['W1'])) # Calculate hidden layer\n",
    "    logp = tf.matmul(h, tf_model['W2']) # Calculates output value\n",
    "    p = tf.nn.softmax(logp) # Convert out values into a probability i.e. sum of outputs = 1\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Placeholders\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_input], name = \"X\")\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_actions], name = \"Y\")\n",
    "tf_epr = tf.placeholder(dtype = tf.float32, shape=[None,1], name=\"tf_epr\")\n",
    "\n",
    "# Discount rewards\n",
    "tf_discounted_g = tf_discount(tf_epr)\n",
    "# Normalize rewards\n",
    "tf_mean, tf_variance = tf.nn.moments(tf_discounted_g, [0], shift = None, name = \"reward_moments\")\n",
    "tf_discounted_g -=tf_mean\n",
    "tf_discounted_g/= tf.sqrt(tf_variance + 1e-6)\n",
    "\n",
    "# TF optimiser op\n",
    "tf_aprob = tf_policy_forward(X)\n",
    "loss = tf.nn.l2_loss(Y-tf_aprob)\n",
    "optimiser - tf.train.AdamOptimiser(learning_rate, betal = decay1, beta2 = decay2)\n",
    "# optimizer = tf.train.RMSPropOptimizer(learning_rate, decay = decay)\n",
    "tf_grads  = optimizer.compute_gradients(loss, var_list = tf.trainable_variables(), grad_loss = tf_discounted_g)\n",
    "train_op = optimizer.apply_gradients(tf_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Graph initialization\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if exists\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "load_was_success = True\n",
    "try:\n",
    "    save_dir = '/'.join(save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkout_path\n",
    "    saver.restore(sess, load_path)\n",
    "except:\n",
    "    print(\"no saved model to load. starting new session\")\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print(\"loaded model: {}\".format(load_path))\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    episode_number = int(load_path.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = np.zeros([1, n_input])\n",
    "state2 = resize(env.reset())\n",
    "render = False\n",
    "\n",
    "while True:\n",
    "    # Decay learning rate\n",
    "    if n_episode > 4000:\n",
    "        learning_rate = le-5\n",
    "    if n_episode > 5000:\n",
    "        learning_rate = 1e-7\n",
    "    \n",
    "    # Subtract current state from previous state to visualize motion\n",
    "    state = state2 - state1\n",
    "    state1 = state2\n",
    "\n",
    "    # Sample an action based from the network weights\n",
    "    feed = {X: np.reshape(state, (1,n_inputs))}\n",
    "    # Feed state forward through the network, returns an action between 0 and 2\n",
    "    aprob = sess.run(tf_aprob, feed) ; aprob = aprob[0,:]\n",
    "    # Choose random action based upon action probabilities\n",
    "    action = np.random.choice(n_actions, p = aprob)\n",
    "    # Convert action into one hot vector\n",
    "    label = np.zeros_like(aprob) ; label[action] = 1\n",
    "\n",
    "    # Take an action and get new variables, we add 1 to action based upon env.env.get_action\n",
    "    state2, reward, done, info = env.step(action+1)\n",
    "    state2 = resize(state2)\n",
    "    if render:\n",
    "        env.render()\n",
    "    G += reward\n",
    "\n",
    "    # Record state , action & reward history\n",
    "    xs.append(state) ; ys.append(label) ; rs.append(reward)\n",
    "\n",
    "    if done:\n",
    "        # parameter update\n",
    "        feed = {X: np.vstack(xs), tf_epr: np.vstack(rs), Y: np.vstack(ys)}\n",
    "        _ = sess.run(train_op, feed)\n",
    "\n",
    "        # update average reward\n",
    "        average_reward = G if average_reward is None else average_reward * 0.99 + G * 0.01\n",
    "\n",
    "        # print progress console\n",
    "        if n_episode % 10 == 0:\n",
    "            print('Episode: {} Reward: {} Average Reward: {:43f}'.format(n_episode, G, average_reward))\n",
    "        else:\n",
    "            print('Episode: {} Reward: {}'.format(n_episode, G))\n",
    "\n",
    "        # bookkeeping\n",
    "\n",
    "        G = 0 # Reset episode reward\n",
    "        n_episode += 1 # the next episode\n",
    "        state1 = np.zeros(1, n_input)\n",
    "        state2 = resize(env.reset()) # Reset environment\n",
    "        xs, rs, ys = [],[],[] # Reset game history\n",
    "\n",
    "        if n_episode % 50 == 0:\n",
    "            saver.save(sess, save_path, global_step = n_episode)\n",
    "            print(\"SAVED MODEL #{}\".format(n_episode))\n",
    "\n",
    "        if n_episode == 6000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = resize(env.reset())\n",
    "\n",
    "for i in range(50):\n",
    "    state2, _, _, _ = env.step(env.action_space.sample())\n",
    "    state2 = resize(state2)\n",
    "    if i > 15:\n",
    "        plt.lmshow((state2-state1).reshapr(80,80), cmap = 'gray')\n",
    "        plt.show()\n",
    "    state1 = state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = np.zeros([1, n_input])\n",
    "state2 = resize(env.reset())\n",
    "\n",
    "while True:\n",
    "    state = state2-state1\n",
    "    state1 = state2\n",
    "    env.render()\n",
    "    feed = {X: np.reshape(state, (1,n_inputs))}\n",
    "    aprob = sess.run(tf_aprob, feed) ; aprob = aprob[0,:]\n",
    "    action = np.argmax(aprob)+1\n",
    "    state2, reward, done, info = env.step(action)\n",
    "    state2 = resize(state2)\n",
    "\n",
    "    if reward == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightVal = sess.run(tf_model['W1'])\n",
    "\n",
    "for i in range(200):\n",
    "    plt.imshow(weightVal[:,i].reshape(80,80), cmap = 'coolwarm')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
