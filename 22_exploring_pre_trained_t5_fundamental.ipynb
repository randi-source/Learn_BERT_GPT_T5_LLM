{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/randi_eka/anaconda3/envs/huggingface/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 8.24MB/s]\n",
      "model.safetensors: 100%|██████████| 892M/892M [02:52<00:00, 5.16MB/s] \n",
      "generation_config.json: 100%|██████████| 147/147 [00:00<00:00, 1.11MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 818kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.39M/1.39M [00:01<00:00, 1.07MB/s]\n",
      "/home/randi_eka/anaconda3/envs/huggingface/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "base_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "base_tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text preprocessed: \n",
      " Sinan Ozdemir is a data scientist, startup founder, and educator living in the San Francisco Bay with his dog, Charlie; cat, Euclid; and bearded dragon, Fiero. He spent his academic career studying pure mathematics at John Hopkins University before transitioning to education. He spent several years conducting lectures on data science at John Hopkins University and at the General Assembly before founding his own startup, Kylie.ai, which uses artificial intelligence to build chatbots from historical transcripts. After completing a Fellowship at the Y Combinator accelerator, Sinan spend most of his time working on his fast-growing company, while creating educational material for data science.\n"
     ]
    }
   ],
   "source": [
    "text_to_summarize = \"\"\"Sinan Ozdemir is a data scientist, startup founder, and educator living in the San Francisco Bay with his dog, \n",
    "Charlie; cat, Euclid; and bearded dragon, Fiero. He spent his academic career studying pure mathematics \n",
    "at John Hopkins University before transitioning to education. He spent several years conducting lectures \n",
    "on data science at John Hopkins University and at the General Assembly before founding his own startup, \n",
    "Kylie.ai, which uses artificial intelligence to build chatbots from historical transcripts. \n",
    "After completing a Fellowship at the Y Combinator accelerator, Sinan spend most of his time working on \n",
    "his fast-growing company, while creating educational material for data science.\n",
    "\"\"\"\n",
    "\n",
    "preprocess_text = text_to_summarize.strip().replace(\"\\n\", \"\")\n",
    "\n",
    "print(\"original text preprocessed: \\n\", preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text: \n",
      "Sinan Ozdemir is a data scientist, startup founder, and educator. he founded his own startup, Kylie.ai, which uses artificial intelligence to build chatbots.\n"
     ]
    }
   ],
   "source": [
    "# known prompt for summarization with T5\n",
    "t5_prepared_text = \"summarize: \" + preprocess_text\n",
    "\n",
    "input_ids = base_tokenizer.encode(t5_prepared_text, return_tensors=\"pt\")\n",
    "\n",
    "# summarize\n",
    "summary_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams = 4,\n",
    "    no_repeat_ngram_size = 3,\n",
    "    min_length = 30,\n",
    "    max_length = 50,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"Summarized text: \\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English -> German Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text:\n",
      "Wo ist die Schokolade?\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\"translate English to German: Where is the chocolate?\", return_tensors=\"pt\")\n",
    "\n",
    "# translate\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams = 4,\n",
    "    no_repeat_ngram_size = 3,\n",
    "    max_length = 20,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"Translated text:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3488,   229,    67, 31267,    58,     1]]),\n",
       " tensor(0.1136, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass labels in to calculate loss\n",
    "\n",
    "input_ids = base_tokenizer('translate English to German: Where is the chocolate?', return_tensors=\"pt\").input_ids\n",
    "labels = base_tokenizer('Wo ist die Schokolade?', return_tensors = 'pt').input_ids\n",
    "\n",
    "loss = base_model(input_ids = input_ids, labels = labels).loss\n",
    "\n",
    "labels, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoLA: The Corpus of Linguistic Acceptability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is grammatically correct?: \n",
      "acceptable\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode('cola sentence: Where is the chocolate?', return_tensors = 'pt')\n",
    "\n",
    "#CoLA\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams = 4,\n",
    "    no_repeat_ngram_size = 3,\n",
    "    max_length = 20,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"is grammatically correct?: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is grammatically correct?: \n",
      "unacceptable\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode('cola sentence: Where be a chocolate?', return_tensors = 'pt')\n",
    "\n",
    "#CoLA\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams = 4,\n",
    "    no_repeat_ngram_size = 3,\n",
    "    max_length = 20,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"is grammatically correct?: \\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STSB - Semantic Text Similarity Benchmark\n",
    "Are two sentences semantically similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantically similar? (1-5): \n",
      "3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/randi_eka/anaconda3/envs/huggingface/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentence_one = \"How to fish\"\n",
    "sentence_two = \"Fishing Manual for beginners\"\n",
    "\n",
    "input_ids = base_tokenizer.encode(f\"stsb sentence1: {sentence_one} sentence2: {sentence_two}\", return_tensors='pt')\n",
    "\n",
    "# calculate semantic similarity\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length = 3,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"semantically similar? (1-5): \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantically similar? (1-5): \n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "sentence_one = \"How to fish\"\n",
    "sentence_two = \"Hiking Manual for beginners\"\n",
    "\n",
    "input_ids = base_tokenizer.encode(f\"stsb sentence1: {sentence_one} sentence2: {sentence_two}\", return_tensors='pt')\n",
    "\n",
    "# calculate semantic similarity\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length = 3,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"semantically similar? (1-5): \\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI - Multi-Genre Natural Language Inference\n",
    "Whether a premise implies (\"entailment\"), contradicts (\"contradiction\"), or neither (\"neutral\") a hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "en\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(f\"mnli premise: I am active in politics. hypothesis: I am running for mayor\", return_tensors='pt')\n",
    "\n",
    "# mnli\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length = 3,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "contradiction\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(f\"mnli premise: I am active in politics. hypothesis: I do not really vote\", return_tensors='pt')\n",
    "\n",
    "# mnli\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length = 3,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(f\"mnli premise: I am active in politics. hypothesis: I code for a living\", return_tensors='pt')\n",
    "\n",
    "# mnli\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length = 3,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q/A - Question/Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "California\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    \"question: Where does Sinan live? context: Sinan lives in California but Matt lives in Boston\", return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "Boston\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    \"question: Where does Matt live? context: Sinan lives in California but Matt lives in Boston\", return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "not_duplicate\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    \"prompt1: Where does Matt live? prompt2: Sinan lives in California but Matt lives in Boston\", return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens = True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
