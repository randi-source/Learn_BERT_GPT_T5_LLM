{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/randi_eka/anaconda3/envs/huggingface/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-31 20:09:47.694380: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-31 20:09:47.694443: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-31 20:09:47.694502: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-31 20:09:47.708156: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-31 20:09:48.474668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification, DistilBertForTokenClassification, \\\n",
    "    DistilBertTokenizerFast, pipeline, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import load_metric, Dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'listen O\\r\\n',\n",
       " b'to O\\r\\n',\n",
       " b'westbam B-artist\\r\\n',\n",
       " b'alumb O\\r\\n',\n",
       " b'allergic B-album\\r\\n',\n",
       " b'on O\\r\\n',\n",
       " b'google B-service\\r\\n',\n",
       " b'music I-service\\r\\n',\n",
       " b'PlayMusic\\r\\n',\n",
       " b'\\r\\n',\n",
       " b'add O\\r\\n',\n",
       " b'step B-entity_name\\r\\n',\n",
       " b'to I-entity_name\\r\\n',\n",
       " b'me I-entity_name\\r\\n',\n",
       " b'to O\\r\\n',\n",
       " b'the O\\r\\n',\n",
       " b'50 B-playlist\\r\\n',\n",
       " b'cl\\xc3\\xa1sicos I-playlist\\r\\n',\n",
       " b'playlist O\\r\\n',\n",
       " b'AddToPlaylist\\r\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_file = open('snips.train.txt', 'rb')\n",
    "\n",
    "snips_rows = snips_file.readlines()\n",
    "\n",
    "snips_rows[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code segment parses the snips dataset into a more manageable format\n",
    "\n",
    "utterances = []\n",
    "tokenized_utterances = []\n",
    "labels_for_tokens = []\n",
    "sequence_labels = []\n",
    "\n",
    "utterance, tokenized_utterance, label_for_utterances = '', [], []\n",
    "for snip_row in snips_rows:\n",
    "  if len(snip_row) == 2: # skip over rows with no data\n",
    "    continue\n",
    "  if ' ' not in snip_row.decode(): # we've hit a sequence label\n",
    "    sequence_labels.append(snip_row.decode().strip())\n",
    "    utterances.append(utterance.strip())\n",
    "    tokenized_utterances.append(tokenized_utterance)\n",
    "    labels_for_tokens.append(label_for_utterances)\n",
    "    utterance = ''\n",
    "    tokenized_utterance = []\n",
    "    label_for_utterances = []\n",
    "    continue\n",
    "  token, token_label = snip_row.decode().split(' ')\n",
    "  token_label = token_label.strip()\n",
    "  utterance += f'{token} '\n",
    "  tokenized_utterance.append(token)\n",
    "  label_for_utterances.append(token_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RateBook',\n",
       " 'SearchScreeningEvent',\n",
       " 'SearchCreativeWork',\n",
       " 'PlayMusic',\n",
       " 'AddToPlaylist',\n",
       " 'BookRestaurant',\n",
       " 'GetWeather']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sequence_labels = list(set(sequence_labels))\n",
    "unique_sequence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 unique sequence labels\n"
     ]
    }
   ],
   "source": [
    "sequence_labels = [unique_sequence_labels.index(l) for l in sequence_labels]\n",
    "print(f\"There are {len(unique_sequence_labels)} unique sequence labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 72 unique token labels\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "unique_token_labels = list(set(reduce(lambda x, y: x+y, labels_for_tokens)))\n",
    "labels_for_tokens = [[unique_token_labels.index(_) for _ in l] for l in labels_for_tokens]\n",
    "\n",
    "print(f\"There are {len(unique_token_labels)} unique token labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "snips_dataset = Dataset.from_dict( # hold data for both sequence and token classification\n",
    "    dict(\n",
    "        utterance = utterances,\n",
    "        label = sequence_labels,\n",
    "        tokens = tokenized_utterances,\n",
    "        token_labels = labels_for_tokens\n",
    "    )\n",
    ")\n",
    "snips_dataset = snips_dataset.train_test_split(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'i d like to see forgetting the girl',\n",
       " 'label': 1,\n",
       " 'tokens': ['i', 'd', 'like', 'to', 'see', 'forgetting', 'the', 'girl'],\n",
       " 'token_labels': [5, 5, 5, 5, 5, 68, 51, 51]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our dataset from the sequence classification section\n",
    "snips_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 1040, 2066, 2000, 2156, 17693, 1996, 2611, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(snips_dataset['train'][0][\"tokens\"], truncation = True, is_split_into_words = True)\n",
    "tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'restaurant',\n",
       " 'in',\n",
       " 'kentucky',\n",
       " 'for',\n",
       " 'ten',\n",
       " 'at',\n",
       " '0',\n",
       " 'am',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 2054, 2003, 1996, 2190, 4825, 1999, 5612, 2005, 2702, 2012, 1014, 2572, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs.word_ids(batch_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The given \"token_labels\" may not match up with the BERT wordpiece tokenization so\n",
    "#   this function will map them to the tokenization that BERT uses\n",
    "#   -100 is a reserved for labels where we do not want to calculate losses so BERT doesn't waste time\n",
    "#   trying to predict tokens like CLS or SEP\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation = True, is_split_into_words = True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"token_labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None: # Set any special tokens / punctuation to -100\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx: # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100) # everything else labeled as -100\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'i d like to see forgetting the girl',\n",
       " 'label': 1,\n",
       " 'tokens': ['i', 'd', 'like', 'to', 'see', 'forgetting', 'the', 'girl'],\n",
       " 'token_labels': [5, 5, 5, 5, 5, 68, 51, 51]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10467/10467 [00:00<00:00, 25048.06 examples/s]\n",
      "Map: 100%|██████████| 2617/2617 [00:00<00:00, 28406.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Map our dataset from sequence classification to be for token classification\n",
    "tok_clf_tokenized_snips = snips_dataset.map(tokenize_and_align_labels, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'i d like to see forgetting the girl',\n",
       " 'label': 1,\n",
       " 'tokens': ['i', 'd', 'like', 'to', 'see', 'forgetting', 'the', 'girl'],\n",
       " 'token_labels': [5, 5, 5, 5, 5, 68, 51, 51],\n",
       " 'input_ids': [101, 1045, 1040, 2066, 2000, 2156, 17693, 1996, 2611, 102],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 5, 5, 5, 5, 5, 68, 51, 51, -100]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_clf_tokenized_snips['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_clf_tokenized_snips['train'] = tok_clf_tokenized_snips['train'].remove_columns(\n",
    "    ['utterance', 'label', 'tokens', 'token_labels']\n",
    ")\n",
    "\n",
    "tok_clf_tokenized_snips['test'] = tok_clf_tokenized_snips['test'].remove_columns(\n",
    "    ['utterance', 'label', 'tokens', 'token_labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10467\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2617\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_clf_tokenized_snips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tok_clf_model = DistilBertForTokenClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', num_labels = len(unique_token_labels)\n",
    ")\n",
    "\n",
    "# Set our label dictionary\n",
    "tok_clf_model.config.id2label = {i: l for i, l in enumerate(unique_token_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I-object_location_type', 'B-best_rating')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_clf_model.config.id2label[0], tok_clf_model.config.id2label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./snips_tok_clf/results\",\n",
    "    num_train_epochs = epochs,\n",
    "    per_device_train_batch_size =32,\n",
    "    per_device_eval_batch_size =32,\n",
    "    load_best_model_at_end = True,\n",
    "\n",
    "    logging_steps = 10,\n",
    "    log_level = 'info',\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch'\n",
    ")\n",
    "\n",
    "#Define the trainer:\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = tok_clf_model,\n",
    "    args = training_args,\n",
    "    train_dataset = tok_clf_tokenized_snips['train'],\n",
    "    eval_dataset = tok_clf_tokenized_snips['test'],\n",
    "    data_collator = tok_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{0: 'I-object_location_type', 1: 'B-best_rating', 2: 'I-playlist', 3: 'B-year', 4: 'I-object_part_of_series_type', 5: 'O', 6: 'B-service', 7: 'B-spatial_relation', 8: 'I-restaurant_type', 9: 'B-object_location_type', 10: 'I-current_location', 11: 'B-movie_type', 12: 'I-facility', 13: 'B-timeRange', 14: 'B-poi', 15: 'B-object_type', 16: 'B-cuisine', 17: 'B-location_name', 18: 'B-object_name', 19: 'I-restaurant_name', 20: 'I-party_size_description', 21: 'B-country', 22: 'I-city', 23: 'B-party_size_description', 24: 'B-restaurant_name', 25: 'I-served_dish', 26: 'B-track', 27: 'I-object_name', 28: 'I-album', 29: 'B-artist', 30: 'I-entity_name', 31: 'B-object_part_of_series_type', 32: 'B-genre', 33: 'B-served_dish', 34: 'B-rating_value', 35: 'B-geographic_poi', 36: 'B-sort', 37: 'B-city', 38: 'B-condition_description', 39: 'B-entity_name', 40: 'I-service', 41: 'I-spatial_relation', 42: 'B-current_location', 43: 'I-location_name', 44: 'I-object_type', 45: 'I-geographic_poi', 46: 'B-party_size_number', 47: 'I-sort', 48: 'B-playlist', 49: 'B-playlist_owner', 50: 'B-album', 51: 'I-movie_name', 52: 'I-timeRange', 53: 'I-movie_type', 54: 'B-rating_unit', 55: 'B-facility', 56: 'I-country', 57: 'I-state', 58: 'I-track', 59: 'B-restaurant_type', 60: 'I-cuisine', 61: 'B-music_item', 62: 'I-object_select', 63: 'I-genre', 64: 'B-object_select', 65: 'I-playlist_owner', 66: 'I-music_item', 67: 'I-artist', 68: 'B-movie_name', 69: 'I-poi', 70: 'B-state', 71: 'B-condition_temperature'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n",
      "Trainer is attempting to log a value of \"{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7, 'LABEL_8': 8, 'LABEL_9': 9, 'LABEL_10': 10, 'LABEL_11': 11, 'LABEL_12': 12, 'LABEL_13': 13, 'LABEL_14': 14, 'LABEL_15': 15, 'LABEL_16': 16, 'LABEL_17': 17, 'LABEL_18': 18, 'LABEL_19': 19, 'LABEL_20': 20, 'LABEL_21': 21, 'LABEL_22': 22, 'LABEL_23': 23, 'LABEL_24': 24, 'LABEL_25': 25, 'LABEL_26': 26, 'LABEL_27': 27, 'LABEL_28': 28, 'LABEL_29': 29, 'LABEL_30': 30, 'LABEL_31': 31, 'LABEL_32': 32, 'LABEL_33': 33, 'LABEL_34': 34, 'LABEL_35': 35, 'LABEL_36': 36, 'LABEL_37': 37, 'LABEL_38': 38, 'LABEL_39': 39, 'LABEL_40': 40, 'LABEL_41': 41, 'LABEL_42': 42, 'LABEL_43': 43, 'LABEL_44': 44, 'LABEL_45': 45, 'LABEL_46': 46, 'LABEL_47': 47, 'LABEL_48': 48, 'LABEL_49': 49, 'LABEL_50': 50, 'LABEL_51': 51, 'LABEL_52': 52, 'LABEL_53': 53, 'LABEL_54': 54, 'LABEL_55': 55, 'LABEL_56': 56, 'LABEL_57': 57, 'LABEL_58': 58, 'LABEL_59': 59, 'LABEL_60': 60, 'LABEL_61': 61, 'LABEL_62': 62, 'LABEL_63': 63, 'LABEL_64': 64, 'LABEL_65': 65, 'LABEL_66': 66, 'LABEL_67': 67, 'LABEL_68': 68, 'LABEL_69': 69, 'LABEL_70': 70, 'LABEL_71': 71}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.33568000793457,\n",
       " 'eval_runtime': 4.382,\n",
       " 'eval_samples_per_second': 597.222,\n",
       " 'eval_steps_per_second': 18.713}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 10,467\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 656\n",
      "  Number of trainable parameters = 66,418,248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='656' max='656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [656/656 00:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.224400</td>\n",
       "      <td>0.183765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.130217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_tok_clf/results/tmp-checkpoint-328\n",
      "Configuration saved in ./snips_tok_clf/results/tmp-checkpoint-328/config.json\n",
      "Model weights saved in ./snips_tok_clf/results/tmp-checkpoint-328/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_tok_clf/results/tmp-checkpoint-656\n",
      "Configuration saved in ./snips_tok_clf/results/tmp-checkpoint-656/config.json\n",
      "Model weights saved in ./snips_tok_clf/results/tmp-checkpoint-656/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./snips_tok_clf/results/checkpoint-656 (score: 0.13021674752235413).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=656, training_loss=0.4274501119081567, metrics={'train_runtime': 50.6854, 'train_samples_per_second': 413.019, 'train_steps_per_second': 12.943, 'total_flos': 116195158657872.0, 'train_loss': 0.4274501119081567, 'epoch': 2.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.13021674752235413,\n",
       " 'eval_runtime': 2.9146,\n",
       " 'eval_samples_per_second': 897.881,\n",
       " 'eval_steps_per_second': 28.134,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-entity_name',\n",
       "  'score': 0.92159677,\n",
       "  'index': 2,\n",
       "  'word': 'two',\n",
       "  'start': 4,\n",
       "  'end': 7},\n",
       " {'entity': 'I-entity_name',\n",
       "  'score': 0.9126515,\n",
       "  'index': 3,\n",
       "  'word': 'coins',\n",
       "  'start': 8,\n",
       "  'end': 13},\n",
       " {'entity': 'I-entity_name',\n",
       "  'score': 0.7290275,\n",
       "  'index': 4,\n",
       "  'word': 'by',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'B-artist',\n",
       "  'score': 0.7227906,\n",
       "  'index': 5,\n",
       "  'word': 'dispatch',\n",
       "  'start': 17,\n",
       "  'end': 25},\n",
       " {'entity': 'B-playlist_owner',\n",
       "  'score': 0.98947173,\n",
       "  'index': 7,\n",
       "  'word': 'my',\n",
       "  'start': 29,\n",
       "  'end': 31},\n",
       " {'entity': 'B-playlist',\n",
       "  'score': 0.99105483,\n",
       "  'index': 8,\n",
       "  'word': 'road',\n",
       "  'start': 32,\n",
       "  'end': 36},\n",
       " {'entity': 'I-playlist',\n",
       "  'score': 0.9905984,\n",
       "  'index': 9,\n",
       "  'word': 'trip',\n",
       "  'start': 37,\n",
       "  'end': 41}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"token-classification\", tok_clf_model, tokenizer = tokenizer, device = 0)\n",
    "pipe('Add Two Coins by Dispatch to my road trip playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-object_name',\n",
       "  'score': 0.98055696,\n",
       "  'index': 2,\n",
       "  'word': 'the',\n",
       "  'start': 5,\n",
       "  'end': 8},\n",
       " {'entity': 'I-object_name',\n",
       "  'score': 0.9891699,\n",
       "  'index': 3,\n",
       "  'word': 'principles',\n",
       "  'start': 9,\n",
       "  'end': 19},\n",
       " {'entity': 'I-object_name',\n",
       "  'score': 0.99332607,\n",
       "  'index': 4,\n",
       "  'word': 'of',\n",
       "  'start': 20,\n",
       "  'end': 22},\n",
       " {'entity': 'I-object_name',\n",
       "  'score': 0.9924115,\n",
       "  'index': 5,\n",
       "  'word': 'data',\n",
       "  'start': 23,\n",
       "  'end': 27},\n",
       " {'entity': 'I-object_name',\n",
       "  'score': 0.99358374,\n",
       "  'index': 6,\n",
       "  'word': 'science',\n",
       "  'start': 28,\n",
       "  'end': 35},\n",
       " {'entity': 'B-rating_value',\n",
       "  'score': 0.9920729,\n",
       "  'index': 7,\n",
       "  'word': '5',\n",
       "  'start': 36,\n",
       "  'end': 37},\n",
       " {'entity': 'B-best_rating',\n",
       "  'score': 0.9142672,\n",
       "  'index': 10,\n",
       "  'word': '5',\n",
       "  'start': 45,\n",
       "  'end': 46}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"token-classification\", tok_clf_model, tokenizer = tokenizer, device = 0)\n",
    "pipe('Rate The Principles of Data Science 5 out of 5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
