{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "print(n_states)\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good does performing randomly do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached goal after 51 episodes with a average return of 0.0196078431372549\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "reward = None\n",
    "done = None\n",
    "\n",
    "g = 0\n",
    "episodes = 0\n",
    "rewardTracker = []\n",
    "\n",
    "while reward !=1:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    g += reward\n",
    "    if done == True:\n",
    "        rewardTracker.append(g)\n",
    "        state = env.reset()\n",
    "        episodes += 1\n",
    "print(\"Reached goal after {} episodes with a average return of {}\".format(episodes, sum(rewardTracker)/len(rewardTracker)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good does previously used Q Learning work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return of 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "Q = np.zeros([n_states, n_actions])\n",
    "rewardTracker = []\n",
    "\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "        action = np.argmax(Q[state])\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action])\n",
    "        G += reward\n",
    "        state = state2\n",
    "    rewardTracker.append(G)\n",
    "\n",
    "print(\"Average return of {}\".format(sum(rewardTracker)/len(rewardTracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not receive a negative reward for each action, all our agent ever does is try action 0 and doesn't explore other actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create epsilon greedy policy that will choose a random action at a given epsilon percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return of 0.0362\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.5\n",
    "episodes = 5000\n",
    "rewardTracker = []\n",
    "\n",
    "Q = np.zeros([n_states, n_states])\n",
    "\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(Q[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        Q[state,action] += alpha * ((reward + (np.max(Q[state2])) - Q[state, action]))\n",
    "        G += reward\n",
    "        state = state2\n",
    "    rewardTracker.append(G)\n",
    "\n",
    "print(\"Average return of {}\".format(sum(rewardTracker)/len(rewardTracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return of 0.5838\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1\n",
    "gamma = 0.95\n",
    "\n",
    "rewardTracker = []\n",
    "Q = np.zeros([n_states, n_states])\n",
    "episodes = 5000\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(Q[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            epsilon -= 10**-3\n",
    "\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        Q[state,action] += alpha * ((reward + gamma * (np.max(Q[state2])) - Q[state, action]))\n",
    "        G += reward\n",
    "        state = state2\n",
    "    rewardTracker.append(G)\n",
    "\n",
    "if (sum(rewardTracker[episode-100:episode])/100.0) > .78:\n",
    "    print('------------------------------------------------')\n",
    "    print('Solved after {} episodes with average return of {}'.format(episodeNum-100, sum(rewardTracker[episodeNum-100]))) \n",
    "\n",
    "print(\"Average return of {}\".format(sum(rewardTracker)/len(rewardTracker)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting closer but we need to define a more appropriate epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(eps, Q, state, episode):\n",
    "    if np.random.rand() > eps:\n",
    "        action = np.argmax(Q[state,:] + np.random.randn(1, n_actions)/(episode/4))\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        eps -= 10**-5\n",
    "\n",
    "    return action, eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like other machine learning problems you need to define optimal hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_Q(alpha, gamma, eps, numTrainingEpisodes, numTrainingSteps):\n",
    "\n",
    "    global Q_star\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    rewardTracker = []\n",
    "\n",
    "    for episode in range(1, numTrainingEpisodes+1):\n",
    "        G = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for step in range(1, numTrainingSteps):\n",
    "            action, eps = e_greedy(eps, Q, state, episode)\n",
    "            state2, reward, done, info = env.step(action)\n",
    "            Q[state, action] += alpha * (reward + gamma * np.max(Q[state2]) - Q[state,action])\n",
    "            state = state2\n",
    "            G += reward\n",
    "        \n",
    "        rewardTracker.append(G)\n",
    "\n",
    "        if episode % (numTrainingEpisodes*.10) == 0 and episode != 0:\n",
    "            print('Alpha {} Gamma {} Epsilon {:04.3f} Episode {} of {}'.format(alpha, gamma, eps, episode, numTrainingEpisodes))\n",
    "            print(\"Average Total Return: {}\".format(sum(rewardTracker)/episode))\n",
    "\n",
    "        if (sum(rewardTracker[episode-100:episode])/100.0) > .78:\n",
    "            print('------------------------------------------------')\n",
    "            print('Solved after {} episodes with average return of {}'.format(episode-100, sum(rewardTracker[episode-100:episodes])))\n",
    "            Q_star = Q\n",
    "            break\n",
    "Q_star = Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.8 Gamma 0.95 Epsilon 0.022 Episode 500 of 5000\n",
      "Average Total Return: 0.048\n",
      "Alpha 0.8 Gamma 0.95 Epsilon 0.005 Episode 1000 of 5000\n",
      "Average Total Return: 0.28\n",
      "Alpha 0.8 Gamma 0.95 Epsilon 0.001 Episode 1500 of 5000\n",
      "Average Total Return: 0.38133333333333336\n",
      "------------------------------------------------\n",
      "Solved after 1781 episodes with average return of 79.0\n"
     ]
    }
   ],
   "source": [
    "# Alpha, Gamma, Eps, Episodes, Steps per Episode\n",
    "learn_Q(0.8, 0.95, 0.1, 5000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Q, numTrainingEpisodes, numTrainingSteps, render):\n",
    "\n",
    "    rewardTracker = []\n",
    "\n",
    "    for episode in range(1, numTrainingEpisodes+1):\n",
    "        G = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for step in range(1, numTrainingSteps):\n",
    "            action = np.argmax(Q[state])\n",
    "            state2, reward, done, info = env.step(action)\n",
    "            state = state2\n",
    "            G += reward\n",
    "            if render == True:\n",
    "                env.render()\n",
    "            \n",
    "            if done == True:\n",
    "                break\n",
    "\n",
    "        rewardTracker.append(G)\n",
    "\n",
    "        if episode % (numTrainingEpisodes*.10) == 0 and episode != 0:\n",
    "            print(\"Average Total Return After {} episodes: {04.3f}\".format(episode, sum(rewardTracker)/episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Q-Table, Number of Episodes, Number of Steps per Episode, Render\n",
    "evaluate(Q_star, 1, 300, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
