{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/randi_eka/anaconda3/envs/huggingface/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-12 22:49:19.320702: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-12 22:49:19.320761: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-12 22:49:19.320779: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-12 22:49:19.324964: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-12 22:49:20.112621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "from random import sample, seed, shuffle\n",
    "from sentence_transformers import InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.3244840204715729,\n",
       " 'start': 400,\n",
       " 'end': 414,\n",
       " 'answer': 'Data Scientist'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERSON = \"Randi Eka Sanjaya\"\n",
    "\n",
    "# Note this is NOT an efficient way to search on google. This is done simply for education purposes\n",
    "google_html = BeautifulSoup(requests.get(f'https://www.google.com/search?q={PERSON}').text).get_text()[:1024]\n",
    "\n",
    "nlp = pipeline(\n",
    "    'question-answering',\n",
    "    model = 'deepset/roberta-base-squad2',\n",
    "    tokenizer = 'deepset/roberta-base-squad2',\n",
    "    max_length = 10\n",
    ")\n",
    "\n",
    "nlp(f'Who is {PERSON}?', google_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 70 documents/paragraphs\n"
     ]
    }
   ],
   "source": [
    "# textbook about insects\n",
    "text = urlopen(\"https://www.gutenberg.org/cache/epub/10834/pg10834.txt\").read().decode()\n",
    "\n",
    "# Only keep documents of at least 100 chatacters\n",
    "documents = list(filter(lambda x: len(x) > 100, text.split('\\r\\n\\r\\n')))\n",
    "\n",
    "documents = np.array(documents)\n",
    "\n",
    "print(f'There are {len(documents)} documents/paragraphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model pre-trained on an asymetric semantic search task\n",
    "# We use the Bi-Encoder to encode all the documents, so that we can use it with semantic search\n",
    "bi_encoder = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "bi_encoder.max_seq_length = 256 # Truncate long documents to 256 tokens\n",
    "\n",
    "bi_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  2.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Documents are encoded by calling model.encode().\n",
    "document_embeddings = bi_encoder.encode(documents, convert_to_tensor = True, show_progress_bar = True)\n",
    "\n",
    "document_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"How many horns does a flea have?\" # a natural language query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 15, 'score': 0.48994922637939453},\n",
       " {'corpus_id': 20, 'score': 0.24793772399425507},\n",
       " {'corpus_id': 22, 'score': 0.1847882866859436}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the query using bi-encoder and find relevant documents\n",
    "question_embedding = bi_encoder.encode(QUESTION, convert_to_tensor = True)\n",
    "\n",
    "# Number of documents to retrieve with the bi-encoder\n",
    "hits = util.semantic_search(question_embedding, document_embeddings, top_k = 3)[0]\n",
    "\n",
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many horns does a flea have?\n",
      "\n",
      "Document 1 Cos_Sim 0.490:\n",
      "\n",
      "When examined by a microscope, the flea is a pleasant object. The body\n",
      "is curiously adorned with a suit of polished armour, neatly jointed, and\n",
      "beset with a great number of sharp pins almost like the quills of a\n",
      "porcupine: it has a small head, large eyes, two horns, or feelers, which\n",
      "proceed from the head, and four long legs from the breast; they are very\n",
      "hairy and long, and have several joints, which fold as it were one\n",
      "within another.\n",
      "\n",
      "\n",
      "Document 2 Cos_Sim 0.248:\n",
      "\n",
      "The Chego is a very small animal, about one fourth the size of a common\n",
      "flea: it is very troublesome, in warm climates, to the poor blacks, such\n",
      "as go barefoot, and the slovenly: it penetrates the skin, under which it\n",
      "lays a bunch of eggs, which swell to the bigness of a small pea.\n",
      "\n",
      "\n",
      "Document 3 Cos_Sim 0.185:\n",
      "\n",
      "\n",
      "This is one of the largest of the insect tribe. It is met with in\n",
      "different countries, and of various sizes, from two or three inches to\n",
      "nearly a foot in length: it somewhat resembles a lobster, and casts its\n",
      "skin, as the lobster does its shell.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Question: {QUESTION}\\n')\n",
    "\n",
    "for i, hit in enumerate(hits):\n",
    "\n",
    "    print(f'Document {i + 1} Cos_Sim {hit[\"score\"]:.3f}:\\n\\n{documents[hit[\"corpus_id\"]]}')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8524739742279053, 'start': 259, 'end': 262, 'answer': 'two'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(QUESTION, str(documents[hits[0]['corpus_id']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is called an \"Open Book Q/A\" System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the adversarial_qa_dataset from the Q/A use-case\n",
    "training_qa = load_dataset('adversarial_qa', 'adversarialQA', split = 'train')\n",
    "\n",
    "good_training_data = []\n",
    "bad_training_data = []\n",
    "\n",
    "last_example = None\n",
    "for example in training_qa:\n",
    "    if last_example and example['context'] != last_example['context']:\n",
    "        bad_training_data.append((example['question'], last_example['context'], float(0))) # add neutral examples\n",
    "    # Question, context, label is 1 if should be matched together\n",
    "    good_training_data.append((example['question'], example['context'], float(1)))\n",
    "    last_example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2647)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_training_data), len(bad_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What letter designates what Ektachrome is designed for?',\n",
       " 'Some high-speed black-and-white films, such as Ilford Delta 3200 and Kodak T-MAX P3200, are marketed with film speeds in excess of their true ISO speed as determined using the ISO testing method. For example, the Ilford product is actually an ISO 1000 film, according to its data sheet. The manufacturers do not indicate that the 3200 number is an ISO rating on their packaging. Kodak and Fuji also marketed E6 films designed for pushing (hence the \"P\" prefix), such as Ektachrome P800/1600 and Fujichrome P1600, both with a base speed of ISO 400.',\n",
       " 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_training_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What film beside Ektachrome and Fujichorme is designed for pushing?',\n",
       " 'The Weston Cadet (model 852 introduced in 1949), Direct Reading (model 853 introduced 1954) and Master III (models 737 and S141.3 introduced in 1956) were the first in their line of exposure meters to switch and utilize the meanwhile established ASA scale instead. Other models used the original Weston scale up until ca. 1955. The company continued to publish Weston film ratings after 1955, but while their recommended values often differed slightly from the ASA film speeds found on film boxes, these newer Weston values were based on the ASA system and had to be converted for use with older Weston meters by subtracting 1/3 exposure stop as per Weston\\'s recommendation. Vice versa, \"old\" Weston film speed ratings could be converted into \"new\" Westons and the ASA scale by adding the same amount, that is, a film rating of 100 Weston (up to 1955) corresponded with 125 ASA (as per ASA PH2.5-1954 and before). This conversion was not necessary on Weston meters manufactured and Weston film ratings published since 1956 due to their inherent use of the ASA system; however the changes of the ASA PH2.5-1960 revision may be taken into account when comparing with newer ASA or ISO values.',\n",
       " 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_training_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sbert.net/docs/training/overview.html for more information on training\n",
    "\n",
    "seed(42) # seed our upcoming sample\n",
    "\n",
    "sampled_training_data = sample(good_training_data, 500) + sample(bad_training_data, 500)\n",
    "\n",
    "shuffle(sampled_training_data) # shuffle our data around\n",
    "\n",
    "training_index = int(.8 * len(sampled_training_data)) # Get an 80/20 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': '',\n",
       " 'texts': ('What changed after the eigth century?',\n",
       "  'There is disagreement about the origin of the term, but general consensus that \"cardinalis\" from the word cardo (meaning \\'pivot\\' or \\'hinge\\') was first used in late antiquity to designate a bishop or priest who was incorporated into a church for which he had not originally been ordained. In Rome the first persons to be called cardinals were the deacons of the seven regions of the city at the beginning of the 6th century, when the word began to mean “principal,” “eminent,” or \"superior.\" The name was also given to the senior priest in each of the \"title\" churches (the parish churches) of Rome and to the bishops of the seven sees surrounding the city. By the 8th century the Roman cardinals constituted a privileged class among the Roman clergy. They took part in the administration of the church of Rome and in the papal liturgy. By decree of a synod of 769, only a cardinal was eligible to become pope. In 1059, during the pontificate of Nicholas II, cardinals were given the right to elect the pope under the Papal Bull In nomine Domini. For a time this power was assigned exclusively to the cardinal bishops, but the Third Lateran Council in 1179 gave back the right to the whole body of cardinals. Cardinals were granted the privilege of wearing the red hat by Pope Innocent IV in 1244.'),\n",
       " 'label': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the training examples\n",
    "train_examples = [InputExample(texts = t[:2], label = t[2]) for t in sampled_training_data[:training_index]]\n",
    "\n",
    "train_examples[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train dataset, a dataloader and the train loss\n",
    "# A data loader is the object that specifically shuffles/grabs batches of data from a Dataset\n",
    "# We don't usually have to explicitly create one using the Trainer because it has a default loader build in\n",
    "train_dataloader = DataLoader(train_examples, shuffle = True, batch_size = 32, collate_fn = bi_encoder.smart_batching_collate)\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(bi_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 36]), torch.Size([32, 256]), torch.Size([32]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(question_batch, context_batch), labels = next(iter(train_dataloader)) # get a sample batch of data\n",
    "\n",
    "question_batch['input_ids'].shape, context_batch['input_ids'].shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation data, sentences1 and sentences2 are lists of questions and context respectively and scores are 0 or 1\n",
    "sentences1, sentences2, scores = zip(*sampled_training_data[training_index:])\n",
    "\n",
    "# Evaluator will evaluate embedding closeness\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5044913287672261"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_encoder.evaluate(evaluator) # Initial evaluation (higher embedding similarity is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 25/25 [00:10<00:00,  2.39it/s]\n",
      "Iteration: 100%|██████████| 25/25 [00:10<00:00,  2.34it/s]\n",
      "Epoch: 100%|██████████| 2/2 [00:23<00:00, 11.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model using the fit method\n",
    "bi_encoder.fit(\n",
    "    train_objectives = [(train_dataloader, train_loss)],\n",
    "    output_path = 'ir/results',\n",
    "    epochs = 2,\n",
    "    evaluator = evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5050109764878448"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_encoder.evaluate(evaluator) # final evaluation (higher embedding similarity is better)\n",
    "# Not a huge jump in performance with 2 epochs. We could try more data or more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fine-tuned IR model\n",
    "finetuned_bi_encoder = SentenceTransformer('ir/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many horns does a flea have?\n",
      "\n",
      "Document 1 Cos_Sim 0.492:\n",
      "\n",
      "When examined by a microscope, the flea is a pleasant object. The body\n",
      "is curiously adorned with a suit of polished armour, neatly jointed, and\n",
      "beset with a great number of sharp pins almost like the quills of a\n",
      "porcupine: it has a small head, large eyes, two horns, or feelers, which\n",
      "proceed from the head, and four long legs from the breast; they are very\n",
      "hairy and long, and have several joints, which fold as it were one\n",
      "within another.\n",
      "\n",
      "\n",
      "Document 2 Cos_Sim 0.250:\n",
      "\n",
      "The Chego is a very small animal, about one fourth the size of a common\n",
      "flea: it is very troublesome, in warm climates, to the poor blacks, such\n",
      "as go barefoot, and the slovenly: it penetrates the skin, under which it\n",
      "lays a bunch of eggs, which swell to the bigness of a small pea.\n",
      "\n",
      "\n",
      "Document 3 Cos_Sim 0.187:\n",
      "\n",
      "\n",
      "This is one of the largest of the insect tribe. It is met with in\n",
      "different countries, and of various sizes, from two or three inches to\n",
      "nearly a foot in length: it somewhat resembles a lobster, and casts its\n",
      "skin, as the lobster does its shell.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Documents are encoded by calling model.encode().\n",
    "document_embeddings = finetuned_bi_encoder.encode(documents, convert_to_tensor = True, show_progress_bar = True)\n",
    "\n",
    "# Encode the query using bi-encoder and find relevant documents\n",
    "question_embedding = finetuned_bi_encoder.encode(QUESTION, convert_to_tensor = True)\n",
    "\n",
    "# Number of documents to retrieve with the bi-encoder\n",
    "hits = util.semantic_search(question_embedding, document_embeddings, top_k = 3)[0]\n",
    "\n",
    "print(f'Question: {QUESTION}\\n')\n",
    "\n",
    "for i, hit in enumerate(hits):\n",
    "\n",
    "    print(f'Document {i + 1} Cos_Sim {hit[\"score\"]:.3f}:\\n\\n{documents[hit[\"corpus_id\"]]}')\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gutenberg_to_documents(guteberg_url, bi_encoder):\n",
    "    text = urlopen(guteberg_url).read().decode()\n",
    "    documents = np.array(list(filter(lambda x: len(x) > 100, text.split('\\r\\n\\r\\n'))))\n",
    "    print(f\"There are {len(documents)} documents/paragraphs\")\n",
    "    return documents, bi_encoder.encode(documents)\n",
    "\n",
    "def retrieve_relevant_documents(bi_encoder, query, documents, document_embeddings, hits = 3):\n",
    "    query_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    hits = util.semantic_search(query_embedding, document_embeddings, top_k = hits)[0]\n",
    "\n",
    "    for i, hit in enumerate(hits):\n",
    "        print(f'Document {i + 1} Cos_Sim {hit[\"score\"]:.3f}:\\n\\n{documents[hit[\"corpus_id\"]]}')\n",
    "        print('\\n')\n",
    "    print(f\"Answer from Top Document: {nlp(query, str(documents[hits[0]['corpus_id']]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1396 documents/paragraphs\n"
     ]
    }
   ],
   "source": [
    "bank_to_basson_documents, banks_to_bassoon_embeddings = gutenberg_to_documents(\n",
    "    'https://www.gutenberg.org/cache/epub/27480/pg27480.txt', finetuned_bi_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Cos_Sim 0.754:\n",
      "\n",
      "BANSHEE (Irish _bean sidhe_; Gaelic _ban sith_, \"woman of the fairies\"), a\n",
      "supernatural being in Irish and general Celtic folklore, whose mournful\n",
      "screaming, or \"keening,\" at night is held to foretell the death of some\n",
      "member of the household visited. In Ireland legends of the banshee belong\n",
      "more particularly to certain families in whose records periodic visits from\n",
      "the spirit are chronicled. A like ghostly informer figures in Brittany\n",
      "folklore. The Irish banshee is held to be the distinction only of families\n",
      "of pure Milesian descent. The Welsh have the banshee under the name _gwrach\n",
      "y Rhibyn_ (witch of Rhibyn). Sir Walter Scott mentions a belief in the\n",
      "banshee as existing in the highlands of Scotland (_Demonology and\n",
      "Witchcraft_, p. 351). A Welsh death-portent often confused with the gwrach\n",
      "y Rhibyn and banshee is the _cyhyraeth_, the groaning spirit.\n",
      "\n",
      "\n",
      "Document 2 Cos_Sim 0.324:\n",
      "\n",
      "BANNU, a town and district of British India, in the Derajat division of the\n",
      "North-West Frontier Province. The town (also called Edwardesabad and\n",
      "Dhulipnagar) lies in the north-west corner of the district, in the valley\n",
      "of the Kurram river. Pop. (1901) 14,300. It forms the base for all punitive\n",
      "expeditions to the Tochi Valley and Waziri frontier.\n",
      "\n",
      "\n",
      "Answer from Top Document: {'score': 0.04472868889570236, 'start': 76, 'end': 94, 'answer': 'supernatural being'}\n"
     ]
    }
   ],
   "source": [
    "retrieve_relevant_documents(finetuned_bi_encoder,\n",
    "    'What is a banshee?',\n",
    "    bank_to_basson_documents,\n",
    "    banks_to_bassoon_embeddings,\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Cos_Sim 0.797:\n",
      "\n",
      "[3] The date 1876 is taken as being that when the Imperial Bank of Germany\n",
      "came into full operation.\n",
      "\n",
      "\n",
      "Document 2 Cos_Sim 0.573:\n",
      "\n",
      "Similar banks had been established in Middelburg, (March 28th, 1616), in\n",
      "Hamburg (1619) and in Rotterdam (February 9th, 1635). Of these the Bank of\n",
      "Hamburg carried on much the largest business and survived the longest. It\n",
      "was not till the 15th of February 1873 that its existence was closed by the\n",
      "act of the German parliament which decreed that Germany should possess a\n",
      "gold standard, and thus removed those conditions of the local medium of\n",
      "exchange--silver coins of very different intrinsic values--whose\n",
      "circulation had provided an ample field for the operations of the bank. The\n",
      "business of the Bank of Hamburg had been conducted in absolute accordance\n",
      "with the regulations under which it was founded.\n",
      "\n",
      "\n",
      "Answer from Top Document: {'score': 0.18934154510498047, 'start': 13, 'end': 17, 'answer': '1876'}\n"
     ]
    }
   ],
   "source": [
    "retrieve_relevant_documents(finetuned_bi_encoder,\n",
    "    'When was the Imperial Bank of Germany founded?',\n",
    "    bank_to_basson_documents,\n",
    "    banks_to_bassoon_embeddings,\n",
    "    2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
