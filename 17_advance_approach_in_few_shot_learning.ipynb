{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, GPT2Tokenizer\n",
    "from torch import tensor\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "generator = pipeline('text-generation', model = 'gpt2', tokenizer = tokenizer)\n",
    "set_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis\n",
      "Text: I hate it when my phone battery dies.\n",
      "Sentiment: Negative\n",
      "###\n",
      "Text: My day has been really great!\n",
      "Sentiment: Positive\n",
      "###\n",
      "Text: Not a fan when it is cloudy\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"\"\"Sentiment Analysis\n",
    "Text: I hate it when my phone battery dies.\n",
    "Sentiment: Negative\n",
    "###\n",
    "Text: My day has been really great!\n",
    "Sentiment: Positive\n",
    "###\n",
    "Text: Not a fan when it is cloudy\n",
    "Sentiment:\"\"\", top_k = 2, temperature = 0.1, max_length = 55)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question/Answering\n",
      "C: Google was founded in the 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control and 56 percent of the stockholder voting power through supervoting stock.\n",
      "Q: When was Google founded?\n",
      "A: 1998\n",
      "###\n",
      "C: Hugging Face is a company which develops social AI-run chatbot applications. It was established in 2016 by Clement Delangue and Julient Chaumond. The company is based in Brooklyn, New York, United States.\n",
      "Q: What does Hugging Face develop?\n",
      "A: social AI-run chatbot applications\n",
      "###\n",
      "C: The New York Jets are a professional American footbal team based in the New York metropolitan area. The Jets compete in the National Footbal League (NFL) as a member club of the league's American Football Conference (AFC) East division\n",
      "Q: What division do the Jets play in?\n",
      "A: The AFC East. The\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"\"\"Question/Answering\n",
    "C: Google was founded in the 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control and 56 percent of the stockholder voting power through supervoting stock.\n",
    "Q: When was Google founded?\n",
    "A: 1998\n",
    "###\n",
    "C: Hugging Face is a company which develops social AI-run chatbot applications. It was established in 2016 by Clement Delangue and Julient Chaumond. The company is based in Brooklyn, New York, United States.\n",
    "Q: What does Hugging Face develop?\n",
    "A: social AI-run chatbot applications\n",
    "###\n",
    "C: The New York Jets are a professional American footbal team based in the New York metropolitan area. The Jets compete in the National Footbal League (NFL) as a member club of the league's American Football Conference (AFC) East division\n",
    "Q: What division do the Jets play in?\n",
    "A:\"\"\", top_k = 2, max_length = 215, temperature = 0.5)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zero Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question/Answering\n",
      "C: The New York Jets are a professional American footbal team based in the New York metropolitan area. The Jets compete in the National Footbal League (NFL) as a member club of the league's American Football Conference (AFC) East division\n",
      "Q: What division do the Jets play in?\n",
      "A: The Jets play in the AFC East, the AFC\n"
     ]
    }
   ],
   "source": [
    "# Same question as before, with no previous examples in Zero-shot learning. Still works\n",
    "print(generator(\"\"\"Question/Answering\n",
    "C: The New York Jets are a professional American footbal team based in the New York metropolitan area. The Jets compete in the National Footbal League (NFL) as a member club of the league's American Football Conference (AFC) East division\n",
    "Q: What division do the Jets play in?\n",
    "A:\"\"\", top_k = 3, max_length = 80, temperature = 0.5)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis\n",
      "Text: This new music video was so good\n",
      "Sentiment: This new music video was so good\n",
      "Sentiment: This new music video was so good\n",
      "Sentiment: This new music video was so good\n",
      "Sentiment: This new music video was\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot doesn't work as much with the sentiment analysis example\n",
    "print(generator(\"\"\"Sentiment Analysis\n",
    "Text: This new music video was so good\n",
    "Sentiment:\"\"\", top_k = 2, temperature = 0.1, max_length = 55)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_summarize = \"\"\"This training will focus on how the GPT family of models are used for NLP tasks including abstractive text summarization and natural language generation. The training will begin with an introduction to neccesary concepts including masked self attention, language models, and transformers and then build on those concepts to introduce the GPT architechure. We will then move into GPT-2 models as well as fine-tunning these models on custom corpora.\n",
    "GPT models are some of the most relevant NLP architectures today and it is closely related to other important NLP deep learning model like BERT. Both of these models are derived from the newly invented transformer architecture and represent an inflection point in how machines process language and context.\n",
    "The Natural Language Processing with Next-Generation Transformer Architecture series of online trainings provides a comprehensive overview of the state-of-the-art natural language processing (NLP) models including GPT and BERT which are derived from the modern attention-driven transformer architecture and the applications these models are used to solve today. All of the trainings in the series blend theory and application through the combination of visual mathematical explanations, strainghtforward applicable Python examples within hands-on jupyter notebook demos, and comprehensive case studies featuring modern problems solvable by NLP models. (Note that at any given time, only a subset of these classes will be scheduled and open for registration.)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization Task:\n",
      "This training will focus on how the GPT family of models are used for NLP tasks including abstractive text summarization and natural language generation. The training will begin with an introduction to neccesary concepts including masked self attention, language models, and transformers and then build on those concepts to introduce the GPT architechure. We will then move into GPT-2 models as well as fine-tunning these models on custom corpora.\n",
      "GPT models are some of the most relevant NLP architectures today and it is closely related to other important NLP deep learning model like BERT. Both of these models are derived from the newly invented transformer architecture and represent an inflection point in how machines process language and context.\n",
      "The Natural Language Processing with Next-Generation Transformer Architecture series of online trainings provides a comprehensive overview of the state-of-the-art natural language processing (NLP) models including GPT and BERT which are derived from the modern attention-driven transformer architecture and the applications these models are used to solve today. All of the trainings in the series blend theory and application through the combination of visual mathematical explanations, strainghtforward applicable Python examples within hands-on jupyter notebook demos, and comprehensive case studies featuring modern problems solvable by NLP models. (Note that at any given time, only a subset of these classes will be scheduled and open for registration.)\n",
      "\n",
      "TL;DR: This training is for those who have been trained in GPS-based NMP architectures for a while and have a good idea of how they can use GPRS to improve their NEPs. This is a great opportunity to learn how to use NPP-like models to train NPs and NTPs, as the NPS and PPT architectures are very similar and can be used in different ways.\n"
     ]
    }
   ],
   "source": [
    "print(generator(\n",
    "    f\"\"\"Summarization Task:\\n{to_summarize}\\nTL;DR:\"\"\",\n",
    "    max_length = 512, top_k = 5, temperature = 0.5, no_repeat_ngram_size = 2\n",
    ")[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
